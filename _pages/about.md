---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

My name is **Shaoyu Liu**, and I am currently a Ph.D. candidate jointly trained by **Xidian University** and the **Qiyuan National Laboratory**, with an expected graduation year of 2027. My advisors are Professor **Guanghui Zhao** (Xidian University) and Professor **Xiangyang Ji** (Qiyuan Lab, Tsinghua University). To date, I have published one first-author paper at **CVPR2025** and have another papers under review at **ICLR2026** and **CVPR2026**. During my studies, I was awarded the Qiyuan National Laboratory Academic Scholarship.
I developed the first event-based vision multimodal large language model, **EventGPT**, which has garnered widespread attention both domestically and internationally (including recognition from the Robotics and Perception Group led by **Davide Scaramuzza** at the University of Zurich, as well as Arizona State University, among others). It was featured in a dedicated article titled **"EventGPT: Giving AI the Power of Superhuman Vision"** by the U.S. developer platform PromptLayer.

My primary research interests include **multimodal large language models**, **neuromorphic vision**, **robotics and perception**, and related fields.


# ğŸ”¥ News
- *2026.01*: &nbsp;ğŸ‰ğŸ‰ One Paper accepted by [**ICLR'26**].
- *2025.11*: &nbsp; [**EventBench**](https://xdusyl.github.io/eventbench.github.io/) Project Page is Now Live!. 
- *2025.11*: &nbsp; Submitted a paper to [**CVPR'26**].
- *2025.09*: &nbsp; Submitted a paper to [**ICLR'26**].
- *2025.05*: &nbsp; Submitted a paper to [**NeurIPS'25**].
- *2025.02*: &nbsp;ğŸ‰ğŸ‰ One Paper accepted by [**CVPR'25**].
- *2024.11*: &nbsp; [**EventGPT**](https://xdusyl.github.io/eventgpt.github.io/) Project Page is Now Live!. 
- *2024.11*: &nbsp; Submitted a paper to [**CVPR'25**]. 

# ğŸ“ Publications 

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2025</div><img src='images/EventGPT.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[EventGPT: Event Stream Understanding with Multimodal Large Language Models](https://openaccess.thecvf.com/content/CVPR2025/papers/Liu_EventGPT_Event_Stream_Understanding_with_Multimodal_Large_Language_Models_CVPR_2025_paper.pdf) <span style="
  font-weight: bold;
  color: #8B0000;
  font-size: 1.2em;
">
  [CVPR2025]
</span>


**Shaoyu Liu**, Jianing Li, Guanghui Zhao, Yunjian Zhang, XinMeng, Xiangyang Ji, Ming Li

[**Project Page**](https://xdusyl.github.io/eventgpt.github.io/) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- The first LLM tailored for event streams, enabling precise temporal reasoning and natural language understanding from asynchronous visual signals.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ArXiV</div><img src='images/EventGPT.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[EventFlash: Towards Efficient MLLMs For Event-Based Vision](https://openreview.net/pdf?id=QuvGqzLwf6) <span style="
  font-weight: bold;
  color: #8B0000;
  font-size: 1.2em;
">
  [ICLR2026]
</span>


**Shaoyu Liu**, Jianing Li, Guanghui Zhao, Yunjian Zhang, Wen Jiang, Ming Li, Xiangyang Ji

<!-- [**Project Page**](https://xdusyl.github.io/eventgpt.github.io/) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong> -->
- The efficient LLM tailored for event streams, enabling efficient temporal reasoning and dynamic scene understanding from asynchronous event stream.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR</div><img src='images/EventFlash.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">



[EventBench: Towards Comprehensive Benchmarking of Event-based MLLMs](https://arxiv.org/abs/2511.18448) <span style="
  font-weight: bold;
  color: #8B0000;
  font-size: 1.2em;
">
  [ArXiv] Submit to CVPR2026
</span>

**Shaoyu Liu**, Jianing Li, Guanghui Zhao, Yunjian Zhang, Xiangyang Ji

[**Project Page**](https://xdusyl.github.io/eventbench.github.io/) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- The comprehensive benchmark for event streams, enabling structured evaluation of understanding, recognition, and spatial reasoning.
</div>
</div>


# ğŸ– Honors and Awards
- *2025.09* ğŸ‰ğŸ‰ Received the Qiyuan National Lab Academic Scholarship
- *2024.09* ğŸ‰ğŸ‰ Received the Qiyuan National Lab Academic Scholarship


# ğŸ’¬ Academic Services
<span style="color:#b52b27"><strong>Conference Reviewer</strong></span>

- IEEE/CVF Conference on Computer Vision and Pattern Recognition (<span style="color:#b52b27">CVPR</span>)
- International Conference on Learning Representations (<span style="color:#b52b27">ICLR</span>)


# ğŸ’» Internships
- *2024.09 - 2025.03*, Research Intern at GuangMing Lab, Shenzhen, China.
- *2024.03 - 2024.08*, Algorithm Intern at Zhizi Xiyuan, Shanghai, China.
